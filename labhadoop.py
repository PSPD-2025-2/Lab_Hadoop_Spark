# -*- coding: utf-8 -*-
"""labhadoop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/160BLmEgto57pch16XLYqWeW493HYfqLh

# **UnB/ESW/PSPD - Laboratório sobre Hadoop (HDFS e MapReduce)**

# 1 Inicializações para o laboratório.
---
"""

# Instalando a biblioteca que permite copiar conteúdos do Gdrive compartilhado do professor
!pip install gdown
import gdown

# Copiando a pasta LabHdoop (material do professor) para o contexto do aluno
url = 'https://drive.google.com/drive/folders/11-Os0P1Pzsniixmo0P91f9AnJcqJqtl0'
gdown.download_folder(url)

!source /content/LabHdoop/colab_env.sh

"""#2 Instalando o Hadoop
---
"""

# Definindo as variáveis de ambiente do Hadoop
import os
os.environ['BASHRC_PATH']= "/root/.bashrc"
os.environ['HADOOP_CONFIG_DIR']="/content/LabHdoop"
os.environ['HADOOP_BASE_DIR']="/content/Hdoop"
os.environ['JAVA_HOME']="/usr/lib/jvm/java-11-openjdk-amd64" # readlink -f /usr/bin/javac
os.environ['HADOOP_CLASSPATH']="/usr/lib/jvm/java-11/openjdk-amd64/lib/tools.jar"
os.environ['HADOOP_HOME']="/content/Hdoop/hadoop"
os.environ['HADOOP_INSTALL']="/content/Hdoop"
os.environ['HADOOP_MAPRED_HOME']="/content/Hdoop/hadoop"
os.environ['HADOOP_COMMON_HOME']="/content/Hdoop/hadoop"
os.environ['HADOOP_HDFS_HOME']="/content/Hdoop/hadoop"
os.environ['YARN_HOME']="/content/Hdoop/hadoop"
os.environ['HADOOP_COMMON_LIB_NATIVE_DIR']="/content/Hdoop/hadoop/lib/native"
os.environ['HADOOP_OPTS']="-Djava.library.path=/content/Hdoop/hadoop/lib/native"

# Criando o diretório onde os arquivos do Hadoop vão ser colocados
!mkdir $HADOOP_BASE_DIR

# Copiando os fonte do hadoop para a pasta $HADOOP_INSTALL_DIR
!cp LabHdoop/hadoop-3.3.6.tar.gz $HADOOP_BASE_DIR
# Opcionalmente pode-se recuperar os arquivos do hadoop pelo comando a seguir
# !wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz -P $HADOOP_INSTALL_DIR

# Descompactando os arquivos do hadoop na pasta $HADOOP_INSTALL_DIR
!tar -xzvf $HADOOP_BASE_DIR/hadoop-3.3.6.tar.gz -C $HADOOP_BASE_DIR
!mv $HADOOP_BASE_DIR/hadoop-3.3.6 $HADOOP_BASE_DIR/hadoop
!rm $HADOOP_BASE_DIR/hadoop-3.3.6.tar.gz

!ls -l $HADOOP_BASE_DIR

# Setup do Hadoop (YARN) para funcionar em modo Single Node (Pseudo-Distribuído)
!cp LabHdoop/hadoop-configs/hadoop-env.sh $HADOOP_HOME/etc/hadoop/
!cp LabHdoop/hadoop-configs/core-site.xml $HADOOP_HOME/etc/hadoop/
!cp LabHdoop/hadoop-configs/hdfs-site.xml $HADOOP_HOME/etc/hadoop/
!cp LabHdoop/hadoop-configs/yarn-site.xml $HADOOP_HOME/etc/hadoop/
!cp LabHdoop/hadoop-configs/mapred-site.xml $HADOOP_HOME/etc/hadoop/

# Ajustando requisitos do Hadoop: ssh sem senha (passo 1)
!rm -rf /root/.ssh
!ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa

# Ajustando requisitos do Hadoop: ssh sem senha (passo 2)
!cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys

# Ajustando requisitos do Hadoop: ssh sem senha (passo 3)
!chmod 0600 /root/.ssh/authorized_keys

# Ajustando requisitos do Hadoop: install do pacote ssh server (passo 4)
!apt install openssh-server
!/etc/init.d/ssh start

"""**Formatando a partição HDFS**"""

# Reset do HDFS (usar essa célula apenas no caso de novos testes de re-criação da partição HDFS)
!$HADOOP_HOME/sbin/stop-all.sh
!rm -rf /tmp/dfsdata/
!rm -rf /tmp/tmpdata/

# Start the NameNode and DataNode
!$HADOOP_HOME/bin/hdfs namenode -format

"""**Ativando o servidor Hadoop**"""

# Iniciando os processos NameNode e DataNode, daemons do HDFS
!$HADOOP_HOME/sbin/start-dfs.sh

# Conferindo os jobs/daemons do HDFS
!jps

# Iniciando os processos relativos ao gerenciador de recursos YARN
!$HADOOP_HOME/sbin/start-yarn.sh

# Conferindo os jobs/daemons do YARN e do HDFS
!jps

"""# 3 Manipulando o HDFS
---
"""

# Listando pasta/diretório na partição HDFS
!$HADOOP_HOME/bin/hdfs dfs -ls /

# Criando pasta/diretório na partição HDFS
!$HADOOP_HOME/bin/hdfs dfs -mkdir /user

# Criando pasta/diretório na partição HDFS
!$HADOOP_HOME/bin/hdfs dfs -mkdir /user/niajus

# Criando pasta/diretório na partição HDFS
!$HADOOP_HOME/bin/hdfs dfs -mkdir /user/niajus/input

# Copiando arquivos do file system local para o HDFS
!$HADOOP_HOME/bin/hdfs dfs -put LabHdoop/textos/poema01.txt /user/niajus/input
!$HADOOP_HOME/bin/hdfs dfs -put LabHdoop/textos/poema02.txt /user/niajus/input

# Listando pasta/diretório na partição HDFS
!$HADOOP_HOME/bin/hdfs dfs -ls /user/niajus/input

# Listando conteúdo de arquivo na partição HDFS
!$HADOOP_HOME/bin/hdfs dfs -cat /user/niajus/input/poema02.txt

"""# 4 Contador de palavras - Versão centralizada/tradicional (sem Hadoop!)
---
"""

# Biblioteca que permite gerar um dicionário de palavras
from collections import Counter

# Conferindo conteúdo do arquivo de palavras a ser usado no experimento
!cat LabHdoop/textos/arqp.txt

# Referenciando o arquivo de teste
file=open("LabHdoop/textos/arqp.txt", "r", encoding="utf-8-sig")

# Leitura python do arquivo de teste
conteudo = file.read()
conteudo

# Dividindo o conteúdo em uma lista de palavras
words = conteudo.split()
words

# Usando a biblioteca Counter para formar um dicionário de ocorrência de palavras
wordcount = Counter(words)
wordcount

# Listando o conteúdo do dicionário
for item in wordcount.items():
    print("{}\t{}".format(*item))

"""#5 Contador de palavras - Versão Python com funções/arquivos para map e reduce
---

**mapper.py**
"""

file=open("LabHdoop/textos/arqp.txt", "r", encoding="utf-8-sig")

for linha in file:
    linha = linha.strip()
    palavras = linha.split()
    for count in palavras:
        print(('%s\t%s') % (count, 1))

"""**Verificando o mapper.py e o arquivo intermediário gerado**"""

!cat LabHdoop/scripts/mapper.py

# Saída normal do mapper.py (testar com o reducer.py)
!cat LabHdoop/textos/arqp.txt | python LabHdoop/scripts/mapper.py > arq_intermediario.txt
!cat arq_intermediario.txt

"""**reducer.py**"""

file=open("arq_intermediario.txt", "r", encoding="utf-8-sig")

palavra_anterior = None
count_anterior = 0
palavra = None

for linha in file:
    linha = linha.strip() # Remove espaços em branco
    palavra, count = linha.split('\t', 1) # Pega o que foi passado pelo mapper.py
    # Tenta converter o count string para um inteiro
    try:
        count = int(count)
    except ValueError:  # Se o valor de count não for int ignora e continua
        continue

    if palavra == palavra_anterior: # Este if só funciona p entradas ordenadas
        count_anterior += count     # (No Hadoop o shuffler faz esse trabalho)
    else:
        if palavra_anterior:
            print (('%s\t%s') % (palavra_anterior, count_anterior)) # Escreve o resultado na saída
        count_anterior = count
        palavra_anterior = palavra

if palavra == palavra_anterior:
    print (('%s\t%s') % (palavra_anterior, count_anterior))

"""**Testando o reducer.py com simulação do shuffle/sort**"""

# Conferindo o conteúdo do script reducer.py
!cat LabHdoop/scripts/reducer.py

# Verificando processamento mapper.py --> reducer.py
!cat LabHdoop/textos/arqp.txt | python LabHdoop/scripts/mapper.py | python LabHdoop/scripts/reducer.py

# Saída ordenada (simulando o shuffle/sort do Hadoop) do mapper.py
!cat LabHdoop/textos/arqp.txt | python LabHdoop/scripts/mapper.py | sort > arq_intermediario.txt
!cat arq_intermediario.txt

# Verificando processamento mapper.py --> shuffle/sort --> reducer.py
!cat LabHdoop/textos/arqp.txt | python LabHdoop/scripts/mapper.py | sort | python LabHdoop/scripts/reducer.py

"""# 6 Contador de palavras -  Versão Hadoop com Java
---
"""

#Inserindo o arquivo de palavras no HDFS
!$HADOOP_HOME/bin/hdfs dfs -put LabHdoop/textos/arqp.txt /user/niajus/input

#Copiando o arquivo java com as classes map e reduce
!cp LabHdoop/scripts/WordCount.java .

# Criando as classes java do contador de palavras
!$HADOOP_HOME/bin/hadoop com.sun.tools.javac.Main WordCount.java

# Empacotando as classes java em wc.jar
!jar cf wc.jar WordCount*.class

# Resetando a pasta HDFS de saída (onde os resultados vão ser depositados)
!$HADOOP_HOME/bin/hdfs dfs -rm /user/niajus/output/*
!$HADOOP_HOME/bin/hdfs dfs -rmdir /user/niajus/output

# Executando o hadoop com o WordCount, versão java
# Formato: hadoop jar pacote.jar Classe pasta-HDFS-origem pasta-HDFS-destino
!$HADOOP_HOME/bin/hadoop jar wc.jar WordCount /user/niajus/input/arqp.txt /user/niajus/output

#Conferindo o resultado do processamento...
!$HADOOP_HOME/bin/hdfs dfs -ls /user/niajus/output/

!$HADOOP_HOME/bin/hdfs dfs -cat /user/niajus/output/*

# Copiando do HDFS para um arquivo no file system local
!rm arq_palavras.txt
!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /user/niajus/output/part-r-00000 /content/arq_palavras.txt

# Listando o conteúdo do arquivo-resultado, copiado do HDFS
!cat arq_palavras.txt

"""# 7 Contador de palavras - Versão Hadoop com Python
---

"""

# Conferindo o conteúdo do arquivo de entrada (já inserido no HDFS)
!$HADOOP_HOME/bin/hdfs dfs -cat /user/niajus/input/poema02.txt

# Copiando hadoop-streaming, necessário para rodar aplicações python no Hadoop (lembrar que python não é linguagem nativa do Hadoop)
!cp $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar .
# Copiando os scripts mapper.py e reducer.py p a pasta local
!cp LabHdoop/scripts/mapper.py .
!cp LabHdoop/scripts/reducer.py .
# Ativando modo de execução para os scripts
!chmod +x *.py
!ls -l

# Preparando a pasta HDFS de saída, que deve estar vazia/não-criada
!$HADOOP_HOME/bin/hdfs dfs -rm /user/niajus/output/*
!$HADOOP_HOME/bin/hdfs dfs -rmdir /user/niajus/output

# Executando o hadoop com o WordCount, versão python
# Formato: hadoop jar path/pacote.jar -input pasta-HDFS-origem -output pasta-HDFS-destino -mapper path/mapper.py -reducer path/reducer.py
!$HADOOP_HOME/bin/hadoop jar /content/hadoop-streaming-3.3.6.jar -input /user/niajus/input/poema02.txt -output /user/niajus/output/ -mapper /content/mapper.py -reducer /content/reducer.py

# Conferindo a pasta de saída
!$HADOOP_HOME/bin/hdfs dfs -ls /user/niajus/output/

# Conferindo o arquivo de saída
!$HADOOP_HOME/bin/hdfs dfs -cat /user/niajus/output/part-00000

# Copiando do HDFS para um arquivo no file system local
!rm /content/arq_palavras.txt
!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /user/niajus/output/part-00000 /content/arq_palavras.txt

# Listando o conteúdo do arquivo-resultado, copiado do HDFS
!cat arq_palavras.txt

"""# Exercício extra-classe
---

<h5>
Imagine um arquivo de entrada com informações sobre juízes e processos em que atuaram, conforme o exemplo ilustrado na Tabela a seguir.

---

| JUIZ_ID    | PROC_ID | STATUS  | TIMESTAMP |
| ---------- | --------| --------|-----------|
|   196      |    242  |   3     | 881250949 |
|   186      |    302  |   3     | 891717742 |
|   196      |    377  |   1     | 878887116 |
|   244      |    051  |   2     | 880606923 |
|   186      |    346  |   1     | 886397596 |
|   118      |    474  |   4     | 884182806 |
|   186      |    265  |   2     | 881171488 |

---

Agora siga as seguintes instruções:
<ol>
  <li> Elabore um programa em Python, em esquema tradicional (não-MapReduce), para ler um arquivo com o formato desta tabela e listar todos os processos (PROC_ID) e situação (STATUS) que cada juíz (JUIZ_ID) participou. Fique atento ao tipo de estrutura que utilizou para resolver o problema; imagine como poderia distribuir o trabalho entre vários *workers* considerando a estrutura de código que você desenhou. <p>

  <li> Monte uma segunda versão considerando as funções Map e Reduce em arquivos python separados, de modo que a saída da função Map seja entrada para a função Reduce <p>

  <li> Coloque a segunda versão (MapReduce) para ser executada em ambiente Hadoop. Fique atento aos passos que teve que realizar para garantir a execução em ambiente Hadoop; Realize testes em arquivos grandes para ter certeza de que o framework suporta BigData <p>

  <li> Entre no ambiente do curso (Moodle) e promova a entrega do notebook relativo a este exercício
</ol>
</h5>
"""